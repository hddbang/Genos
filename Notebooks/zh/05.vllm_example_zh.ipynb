{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 vLLM 提取嵌入\n",
    "\n",
    "我们可以在 GitHub 上获取 vllm 代码：https://github.com/vllm-project/vllm ；官方网站：https://vllm.ai/\n",
    "\n",
    "从大语言模型中提取嵌入时，vLLM 库相比传统的 HuggingFace 方法具有几个显著优势。以下是详细对比：\n",
    "\n",
    "1. **显著更快的推理速度**\n",
    "\n",
    "   vLLM 针对高吞吐量推理进行了优化。它利用先进的调度、连续批处理和更高效的内存管理来最大化 GPU 利用率，从而实现更快的嵌入提取，特别是在处理大批量或大量文本时。\n",
    "\n",
    "2. **更好的 GPU 内存效率**\n",
    "\n",
    "   vLLM 使用 PagedAttention 和内存分页技术来最小化冗余内存分配，使其能够处理更多请求和更大的批次，而不会像 HuggingFace 的默认管道那样快速遇到内存不足的问题。这使得更大的模型（如 10B+ 参数）能够在可用硬件上更流畅地运行。\n",
    "\n",
    "3. **卓越的多 GPU 支持**\n",
    "\n",
    "   通过内置的张量并行支持和高效的多 GPU 调度，vLLM 可以自动在多个 GPU 之间分割模型和工作负载，最大限度地减少瓶颈和手动配置。HuggingFace 加速需要更多手动设置，并且在高吞吐量嵌入提取方面通常扩展性较差。\n",
    "\n",
    "4. **长上下文的更高吞吐量**\n",
    "\n",
    "   vLLM 专门设计用于高效处理长输入序列，使其在从长文档或段落中提取嵌入时更加适合，而 HuggingFace 方法在这些场景中可能会受到内存限制或变得缓慢。\n",
    "\n",
    "5. **易于部署**\n",
    "\n",
    "   vLLM 提供开箱即用的 API 和 HTTP 服务器支持，用于生产推理，允许快速大规模部署。将 HuggingFace 模型集成到生产服务管道中通常需要额外的工程工作才能实现类似的吞吐量和稳定性。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "   random.seed(seed)\n",
    "   np.random.seed(seed)\n",
    "   os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "   torch.manual_seed(seed)\n",
    "   torch.cuda.manual_seed(seed)\n",
    "   torch.cuda.manual_seed_all(seed)\n",
    "   torch.backends.cudnn.deterministic = True\n",
    "   torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试环境：\n",
      "PyTorch 版本: 2.7.1+cu128\n",
      "vllm 版本: 0.10.1.dev1+gbcc0a3cbe\n",
      "CUDA 可用: True\n",
      "CCUDA 版本: 12.8\n",
      "cuDNN 版本: 90701\n",
      "\n",
      "GPU 信息：\n",
      "  GPU 0: NVIDIA A40, 总内存: 46068 MiB, 空闲内存: 45403 MiB\n",
      "  GPU 1: NVIDIA A40, 总内存: 46068 MiB, 空闲内存: 45403 MiB\n",
      "检测到 2 个 GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import subprocess\n",
    "import vllm\n",
    "\n",
    "def get_gpu_info():\n",
    "    try:\n",
    "        cmd = \"nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader\"\n",
    "        result = subprocess.check_output(cmd, shell=True, encoding='utf-8').strip().split('\\n')\n",
    "        print(\"GPU 信息：\")\n",
    "        for idx, line in enumerate(result):\n",
    "            name, total_mem, free_mem = [s.strip() for s in line.split(',')]\n",
    "            print(f\"  GPU {idx}: {name}, 总内存: {total_mem}, 空闲内存: {free_mem}\")\n",
    "        print(f\"检测到 {len(result)} 个 GPU\")\n",
    "    except Exception as e:\n",
    "        print(\"获取 GPU 信息失败：\", e)\n",
    "\n",
    "def print_env_info():\n",
    "    print(f\"PyTorch 版本: {torch.__version__}\")\n",
    "    print(f\"vllm 版本: {vllm.__version__}\")\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    print(f\"CUDA 可用: {cuda_available}\")\n",
    "    if cuda_available:\n",
    "        print(f\"CUDA 版本: {torch.version.cuda}\")\n",
    "        print(f\"cuDNN 版本: {torch.backends.cudnn.version()}\")\n",
    "print('测试环境：')\n",
    "print_env_info()\n",
    "print()\n",
    "get_gpu_info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 传统嵌入提取方法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cea3e18448034aa1ba753aace6208c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最后隐藏层维度: torch.Size([1, 4, 4096])\n",
      "平均池化嵌入维度: torch.Size([1, 4096])\n",
      "平均池化嵌入: [[-0.7643822  -0.12757319  0.5734206  ... -0.12371235 -0.00170616\n",
      "  -0.33227795]]\n",
      "嵌入提取时间: 0.7419 秒\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# 选择设备映射以使用 2 个 GPU\n",
    "\n",
    "model_path = \"/path/to/Genos-10B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\" if torch.cuda.device_count() >= 2 else None\n",
    ")\n",
    "\n",
    "text = \"ATCG\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "    print(f\"最后隐藏层维度: {last_hidden_state.shape}\")\n",
    "\n",
    "    # MEAN 池化\n",
    "    if \"attention_mask\" in inputs:\n",
    "        mask = inputs[\"attention_mask\"].unsqueeze(-1)  # [batch, seq, 1]\n",
    "        masked_hidden = last_hidden_state * mask\n",
    "        sum_hidden = masked_hidden.sum(dim=1)\n",
    "        lengths = mask.sum(dim=1)  # [batch, 1]\n",
    "        mean_pooled = sum_hidden / lengths\n",
    "    else:\n",
    "        # 没有注意力掩码时，对所有 token 求平均\n",
    "        mean_pooled = last_hidden_state.mean(dim=1)\n",
    "\n",
    "    print(f\"平均池化嵌入维度: {mean_pooled.shape}\")\n",
    "    print(f\"平均池化嵌入: {mean_pooled.cpu().numpy()}\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "print(f\"嵌入提取时间: {elapsed:.4f} 秒\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLLM 嵌入提取\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-17 01:29:32 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 12-17 01:29:44 [config.py:3440] Downcasting torch.float32 to torch.bfloat16.\n",
      "INFO 12-17 01:29:44 [config.py:1604] Using max model len 131072\n",
      "INFO 12-17 01:29:48 [config.py:4628] Only \"last\" pooling supports chunked prefill and prefix caching; disabling both.\n",
      "INFO 12-17 01:29:48 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 12-17 01:29:48 [core.py:71] Initializing a V1 LLM engine (v0.10.1.dev1+gbcc0a3cbe) with config: model='/path/to/Genos-10B', speculative_config=None, tokenizer='/path/to/Genos-10B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/path/to/Genos-10B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=PoolerConfig(pooling_type='MEAN', normalize=False, softmax=None, step_tag_id=None, returned_token_ids=None), compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}\n",
      "WARNING 12-17 01:29:48 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 10 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 12-17 01:29:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_bb75a3f2'), local_subscribe_addr='ipc:///tmp/d45b6b00-32dc-479b-b0f6-618b04df0b19', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=6387)\u001b[0;0m INFO 12-17 01:29:51 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_5a2e4a4e'), local_subscribe_addr='ipc:///tmp/3289206b-ebc2-4b9b-a56f-105f4b187ea5', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=6386)\u001b[0;0m INFO 12-17 01:29:51 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_facb5291'), local_subscribe_addr='ipc:///tmp/a9e8c7f1-b6ae-4fbf-b8d3-dd9b8af96208', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=6387)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=6386)\u001b[0;0m INFO 12-17 01:29:52 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 12-17 01:29:52 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=6386)\u001b[0;0m INFO 12-17 01:29:52 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=6387)\u001b[0;0m INFO 12-17 01:29:52 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=6387)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=6386)\u001b[0;0m INFO 12-17 01:29:52 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 12-17 01:29:52 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=6386)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=6387)\u001b[0;0m WARNING 12-17 01:29:53 [custom_all_reduce.py:147] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 12-17 01:29:53 [custom_all_reduce.py:147] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=6386)\u001b[0;0m INFO 12-17 01:29:53 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_8f3e27e4'), local_subscribe_addr='ipc:///tmp/cde2971c-0792-4bc8-995a-f1bbd480deed', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=6387)\u001b[0;0m INFO 12-17 01:29:53 [parallel_state.py:1102] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=6387)\u001b[0;0m INFO 12-17 01:29:53 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=6386)\u001b[0;0m INFO 12-17 01:29:53 [parallel_state.py:1102] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=6386)\u001b[0;0m INFO 12-17 01:29:53 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=6387)\u001b[0;0m INFO 12-17 01:29:53 [gpu_model_runner.py:1843] Starting to load model /path/to/Genos-10B...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=6386)\u001b[0;0m INFO 12-17 01:29:53 [gpu_model_runner.py:1843] Starting to load model /path/to/Genos-10B...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=6387)\u001b[0;0m INFO 12-17 01:29:53 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=6386)\u001b[0;0m INFO 12-17 01:29:53 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=6387)\u001b[0;0m INFO 12-17 01:29:53 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=6386)\u001b[0;0m INFO 12-17 01:29:53 [cuda.py:290] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a351858258e64db993b8c9d3f6e56541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/9 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=6387)\u001b[0;0m INFO 12-17 01:30:07 [default_loader.py:262] Loading weights took 13.83 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=6386)\u001b[0;0m INFO 12-17 01:30:07 [default_loader.py:262] Loading weights took 13.87 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=6387)\u001b[0;0m INFO 12-17 01:30:08 [gpu_model_runner.py:1892] Model loading took 10.0644 GiB and 14.017120 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=6386)\u001b[0;0m INFO 12-17 01:30:08 [gpu_model_runner.py:1892] Model loading took 10.0644 GiB and 14.020643 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=6387)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=6386)\u001b[0;0m WARNING 12-17 01:30:10 [fused_moe.py:695] Using default MoE config. Performance might be sub-optimal! Config file not found at /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/configs/E=8,N=4096,device_name=NVIDIA_A40.json\n",
      "WARNING 12-17 01:30:10 [fused_moe.py:695] Using default MoE config. Performance might be sub-optimal! Config file not found at /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/configs/E=8,N=4096,device_name=NVIDIA_A40.json\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=6387)\u001b[0;0m INFO 12-17 01:30:22 [gpu_worker.py:255] Available KV cache memory: 22.02 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=6386)\u001b[0;0m INFO 12-17 01:30:22 [gpu_worker.py:255] Available KV cache memory: 22.02 GiB\n",
      "INFO 12-17 01:30:23 [kv_cache_utils.py:833] GPU KV cache size: 481,120 tokens\n",
      "INFO 12-17 01:30:23 [kv_cache_utils.py:837] Maximum concurrency for 131,072 tokens per request: 3.67x\n",
      "INFO 12-17 01:30:23 [kv_cache_utils.py:833] GPU KV cache size: 481,120 tokens\n",
      "INFO 12-17 01:30:23 [kv_cache_utils.py:837] Maximum concurrency for 131,072 tokens per request: 3.67x\n",
      "INFO 12-17 01:30:23 [core.py:193] init engine (profile, create kv cache, warmup model) took 15.02 seconds\n",
      "INFO 12-17 01:30:23 [config.py:4628] Only \"last\" pooling supports chunked prefill and prefix caching; disabling both.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from vllm import TokensPrompt\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import os\n",
    "from vllm.config import PoolerConfig\n",
    "from vllm.pooling_params import PoolingParams\n",
    "\n",
    "\n",
    "model_path = \"/path/to/Genos-10B\"\n",
    "seq_length = 128 * 1024 \n",
    "gpu_num = 2\n",
    "\n",
    "llm = LLM(\n",
    "    model=model_path,\n",
    "    trust_remote_code=True,\n",
    "    tensor_parallel_size=gpu_num, \n",
    "    block_size=32,\n",
    "    enable_prefix_caching=True,\n",
    "    enforce_eager=True,\n",
    "    gpu_memory_utilization=0.85,  # 提高 GPU 内存利用率\n",
    "    dtype=torch.bfloat16,\n",
    "    max_model_len=seq_length,\n",
    "    max_num_batched_tokens=seq_length,\n",
    "    override_pooler_config=PoolerConfig(pooling_type=\"MEAN\", normalize=False), # 池化参数，不使用此参数可获取完整的隐藏状态\n",
    "    task='reward',\n",
    "    enable_chunked_prefill=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6f5437e08b844f2804df75c0d1c3205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa317f090b0b4580a4737f6e01c36d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "嵌入提取时间: 0.1067 秒\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "tokenizer = llm.get_tokenizer()\n",
    "seqs = ['ATCG']\n",
    "\n",
    "token_ids = tokenizer(seqs, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "start_time = time.time()\n",
    "outputs = llm.encode(prompt_token_ids=token_ids)\n",
    "pooleds = []\n",
    "for i, output in enumerate(outputs):\n",
    "    pooled = output.outputs.data\n",
    "    pooleds.append(pooled)\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "print(f\"嵌入提取时间: {elapsed:.4f} 秒\")\n",
    "print(pooleds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vllm_mean_pool 和 mean_pooled 之间的 L1 距离: 7.521327\n",
      "vllm_mean_pool 和 mean_pooled 之间的 L2 距离: 0.195607\n",
      "vllm_mean_pool 和 mean_pooled 之间的 Pearson 相关系数: 0.999995\n"
     ]
    }
   ],
   "source": [
    "vllm_mean_pool = pooleds[0]\n",
    "# 计算 vllm_mean_pool 和 mean_pooled 之间的 L1 距离、L2 距离和 Pearson 相关系数\n",
    "\n",
    "l1_distance = torch.norm(vllm_mean_pool.cpu() - mean_pooled.cpu(), p=1).item()\n",
    "l2_distance = torch.norm(vllm_mean_pool.cpu() - mean_pooled.cpu(), p=2).item()\n",
    "\n",
    "# 展平张量以进行相关性计算\n",
    "vllm_mean_flat = vllm_mean_pool.view(-1).cpu().numpy()\n",
    "mean_pooled_flat = mean_pooled.view(-1).cpu().numpy()\n",
    "\n",
    "if vllm_mean_flat.std() == 0 or mean_pooled_flat.std() == 0:\n",
    "    pearson_corr = float('nan')\n",
    "else:\n",
    "    pearson_corr = np.corrcoef(vllm_mean_flat, mean_pooled_flat)[0, 1]\n",
    "\n",
    "print(f\"vllm_mean_pool 和 mean_pooled 之间的 L1 距离: {l1_distance:.6f}\")\n",
    "print(f\"vllm_mean_pool 和 mean_pooled 之间的 L2 距离: {l2_distance:.6f}\")\n",
    "print(f\"vllm_mean_pool 和 mean_pooled 之间的 Pearson 相关系数: {pearson_corr:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 vLLM 进行嵌入提取相比传统方法实现了 **7 倍加速**，同时产生的嵌入与原始输出高度匹配，显著提高了处理效率。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 传统方法无法提取超长序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "159cbd9577e546a29dc9a748160565b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 44.34 GiB of which 10.62 GiB is free. Process 4005586 has 33.71 GiB memory in use. Of the allocated memory 33.39 GiB is allocated by PyTorch, and 12.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m start_time = time.time()\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     last_hidden_state = outputs.last_hidden_state\n\u001b[32m     22\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLast hidden state shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlast_hidden_state.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:1069\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1066\u001b[39m                 module.forward = make_capture_wrapper(module, original_forward, key, specs.index)\n\u001b[32m   1067\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m-> \u001b[39m\u001b[32m1069\u001b[39m outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1070\u001b[39m \u001b[38;5;66;03m# Restore original forward methods\u001b[39;00m\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module, original_forward \u001b[38;5;129;01min\u001b[39;00m monkey_patched_layers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/mixtral/modeling_mixtral.py:459\u001b[39m, in \u001b[36mMixtralModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    456\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     hidden_states = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m MoeModelOutputWithPast(  \u001b[38;5;66;03m# only diff with Mistral is the output type, we need MoE\u001b[39;00m\n\u001b[32m    473\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    474\u001b[39m     past_key_values=past_key_values,\n\u001b[32m    475\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/mixtral/modeling_mixtral.py:322\u001b[39m, in \u001b[36mMixtralDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, position_ids, past_key_value, cache_position, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    321\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m hidden_states, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    333\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/mixtral/modeling_mixtral.py:279\u001b[39m, in \u001b[36mMixtralAttention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[39m\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config._attn_implementation != \u001b[33m\"\u001b[39m\u001b[33meager\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    277\u001b[39m     attention_interface = ALL_ATTENTION_FUNCTIONS[\u001b[38;5;28mself\u001b[39m.config._attn_implementation]\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m attn_output, attn_weights = \u001b[43mattention_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m    \u001b[49m\u001b[43msliding_window\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msliding_window\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# main diff with Llama\u001b[39;49;00m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    291\u001b[39m attn_output = attn_output.reshape(*input_shape, -\u001b[32m1\u001b[39m).contiguous()\n\u001b[32m    292\u001b[39m attn_output = \u001b[38;5;28mself\u001b[39m.o_proj(attn_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/integrations/sdpa_attention.py:81\u001b[39m, in \u001b[36msdpa_attention_forward\u001b[39m\u001b[34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.jit.is_tracing() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(is_causal, torch.Tensor):\n\u001b[32m     79\u001b[39m     is_causal = is_causal.item()\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m attn_output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msdpa_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m attn_output = attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).contiguous()\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 44.34 GiB of which 10.62 GiB is free. Process 4005586 has 33.71 GiB memory in use. Of the allocated memory 33.39 GiB is allocated by PyTorch, and 12.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "text = \"ATCG\" * 32 * 1024 # 128k\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "    print(f\"最后隐藏层形状: {last_hidden_state.shape}\")\n",
    "\n",
    "    # MEAN 池化\n",
    "    if \"attention_mask\" in inputs:\n",
    "        mask = inputs[\"attention_mask\"].unsqueeze(-1)  # [batch, seq, 1]\n",
    "        masked_hidden = last_hidden_state * mask\n",
    "        sum_hidden = masked_hidden.sum(dim=1)\n",
    "        lengths = mask.sum(dim=1)  # [batch, 1]\n",
    "        mean_pooled = sum_hidden / lengths\n",
    "    else:\n",
    "        # 没有注意力掩码时，对所有 token 求平均\n",
    "        mean_pooled = last_hidden_state.mean(dim=1)\n",
    "\n",
    "    print(f\"平均池化嵌入形状: {mean_pooled.shape}\")\n",
    "    print(f\"平均池化嵌入: {mean_pooled.cpu().numpy()}\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "print(f\"嵌入提取时间: {elapsed:.4f} 秒\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vllm 提取超长序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df059f0eeb9d4ee089269a8d30ecb582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e20dbb8ee2e4e6287aff155ac7a1e45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "嵌入提取时间:  21.7921 秒\n",
      "tensor([-0.0255,  0.1597, -0.1451,  ..., -0.0175,  0.0131, -0.1934])\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "tokenizer = llm.get_tokenizer()\n",
    "seqs = ['ATCG' * 32 * 1024]\n",
    "\n",
    "token_ids = tokenizer(seqs, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "start_time = time.time()\n",
    "outputs = llm.encode(prompt_token_ids=token_ids)\n",
    "pooleds = []\n",
    "for i, output in enumerate(outputs):\n",
    "    pooled = output.outputs.data\n",
    "    pooleds.append(pooled)\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "print(f\"嵌入提取时间: {elapsed:.4f} 秒\")\n",
    "print(pooleds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结\n",
    "\n",
    "基于上述实验，我们可以得出以下结论：\n",
    "\n",
    "1. vLLM 相比传统的 HuggingFace 方法实现了显著更快的嵌入提取——大约快 7 倍。\n",
    "\n",
    "2. vLLM 产生的嵌入与常规方法生成的嵌入高度一致，表明 vLLM 可以可靠地用于提高效率。\n",
    "\n",
    "3. vLLM 能够在有限的计算资源内从更长的序列中提取嵌入，使其非常适合需要处理超长输入的下游研究任务。\n",
    "\n",
    "4. 您可以参考上述演示来使用 vllm 提取嵌入。\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
