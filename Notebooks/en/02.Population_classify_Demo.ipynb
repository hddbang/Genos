{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e90b873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import json\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0520d81",
   "metadata": {},
   "source": [
    "### Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0af5b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "# For numpy\n",
    "np.random.seed(seed)\n",
    "# For deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cff7522",
   "metadata": {},
   "source": [
    "### Extract and save embedding\n",
    "##### No truncation is applied here. Please ensure that the input sequence length is ≤ model_max_length (model_max_length = tokenizer.model_max_length)\n",
    "####  No padding is involved during embedding extraction, as all sequences within the batch have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fac544",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSONLDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        super().__init__()\n",
    "        self.file_path = file_path\n",
    "        self.data = []\n",
    "\n",
    "        # read JSONL\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:  \n",
    "                    item = json.loads(line)\n",
    "                    self.data.append(item)\n",
    "\n",
    "        print(f\"[INFO] Loaded {len(self.data)} samples from {file_path}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08448fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_jsonl(dataset_name):\n",
    "    train_dataset = JSONLDataset(f\"./data/{dataset_name}_train.jsonl\")\n",
    "    eval_dataset = JSONLDataset(f\"./data/{dataset_name}_eval.jsonl\")\n",
    "    test_dataset = JSONLDataset(f\"./data/{dataset_name}_test.jsonl\")\n",
    "        \n",
    "    return train_dataset, eval_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18bda2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, tokenizer):\n",
    "    # labels of cage is saved in a string of list\n",
    "    labels = [int(item[\"label\"]) for item in batch]\n",
    "    labels = torch.tensor(labels)\n",
    "    sequences = [item[\"seq\"] for item in batch] \n",
    "    encoding = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    return encoding, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4618e874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Extract embedding\n",
    "# -------------------------------\n",
    "def calc_embeddings(hf_inputs, model, device):\n",
    "    hf_inputs = {k: v.to(device) for k, v in hf_inputs.items()}\n",
    "    mask = hf_inputs.get(\n",
    "        \"attention_mask\", \n",
    "        torch.ones_like(hf_inputs['input_ids'])\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**hf_inputs, output_hidden_states=True)\n",
    "        last_hidden = outputs.hidden_states[-1]  # [B, T, H]\n",
    "        mask = mask.unsqueeze(-1)  # [B, T, 1]\n",
    "        pooled = (last_hidden * mask).sum(1) / mask.sum(1) #mean pooling [B, H]\n",
    "        pooled = pooled.float()  # [B, H]\n",
    "    return pooled\n",
    "\n",
    "def extract_embeddings(model, tokenizer, dataloader, device):\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    for batch in tqdm(dataloader, desc=\"Embedding\"):\n",
    "        inputs, labels = batch\n",
    "        pooled = calc_embeddings(inputs, model, device)\n",
    "        all_embeddings.append(pooled.detach().cpu())\n",
    "        all_labels.append(labels)\n",
    "    return torch.cat(all_embeddings), torch.cat(all_labels)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c8ceea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Save embedding\n",
    "# -------------------------------\n",
    "\n",
    "def _process_embeddings_and_save(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    dataset,\n",
    "    data_save_path,\n",
    "    device,\n",
    "): \n",
    "    data_loader = DataLoader(\n",
    "        dataset, batch_size=1, shuffle=False,\n",
    "        collate_fn=lambda x: collate_fn(x, tokenizer),\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    embedding_data = extract_embeddings(model, tokenizer, data_loader, device)\n",
    "    X, y = embedding_data\n",
    "    torch.save({\n",
    "        \"embeddings\": X.detach().cpu(),\n",
    "        \"labels\": y.detach().cpu(),\n",
    "    }, data_save_path)\n",
    "\n",
    "def extract_embeddings_on_dataset(model, tokenizer, dataset_name, device, embedding_dir, model_name):\n",
    "    train_dataset, eval_dataset, test_dataset = load_dataset_jsonl(dataset_name)\n",
    "    \n",
    "    #columns = test_dataset.column_names\n",
    "    #print(columns)\n",
    "    \n",
    "    if not os.path.exists(embedding_dir):\n",
    "        os.makedirs(f\"{embedding_dir}\")\n",
    "        \n",
    "    data_train_path = f\"{embedding_dir}/{dataset_name}_train.pt\" if os.path.exists(embedding_dir) else f\"./{dataset_name}_train.pt\"\n",
    "    _process_embeddings_and_save(model, tokenizer, train_dataset, data_train_path, device)\n",
    "    \n",
    "    data_eval_path = f\"{embedding_dir}/{dataset_name}_eval.pt\" if os.path.exists(embedding_dir) else f\"./{dataset_name}_eval.pt\"\n",
    "    _process_embeddings_and_save(model, tokenizer, eval_dataset, data_eval_path, device)\n",
    "    \n",
    "    data_test_path = f\"{embedding_dir}/{dataset_name}_test.pt\" if os.path.exists(embedding_dir) else f\"./{dataset_name}_test.pt\"\n",
    "    _process_embeddings_and_save(model, tokenizer, test_dataset, data_test_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd9bac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading model from /data/model/Genos-1.2B\n",
      "[INFO] Loaded 150 samples from ./data/Human_classify_8k_train.jsonl\n",
      "[INFO] Loaded 150 samples from ./data/Human_classify_8k_eval.jsonl\n",
      "[INFO] Loaded 150 samples from ./data/Human_classify_8k_test.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 150/150 [00:12<00:00, 12.19it/s]\n",
      "Embedding: 100%|██████████| 150/150 [00:11<00:00, 12.61it/s]\n",
      "Embedding: 100%|██████████| 150/150 [00:11<00:00, 12.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Embedding extraction main workflow\n",
    "# -------------------------------\n",
    "model_name = \"Genos-1.2B\"\n",
    "model_path = \"/data/model/Genos-1.2B\"\n",
    "datasets = [\"Human_classify_8k\"]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"[INFO] Loading model from {model_path}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_path,\n",
    "    # If the environment does not support flash attention 2, you can comment out the line below.\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "embedding_dir = f\"./embedding/{model_name}/\"\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    extract_embeddings_on_dataset(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        dataset_name,\n",
    "        device,\n",
    "        embedding_dir,\n",
    "        model_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da932e07",
   "metadata": {},
   "source": [
    "### Load the embeddings and train an XGBoost classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba686ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_classifier(X_train, y_train, X_test, random_state=42):\n",
    "    \"\"\"Train XGBoost classifier\"\"\"\n",
    "    X_train_np = X_train.cpu().numpy() if torch.is_tensor(X_train) else X_train\n",
    "    y_train_np = y_train.cpu().numpy() if torch.is_tensor(y_train) else y_train\n",
    "    X_test_np = X_test.cpu().numpy() if torch.is_tensor(X_test) else X_test\n",
    "    \n",
    "    print(\"Training XGBoost classifier...\")\n",
    "    print(\"XGboost parameters: n_estimators=100, learning_rate=0.1, max_depth=6, random_state={}\".format(random_state))\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=random_state,\n",
    "        # use_label_encoder=False, \n",
    "        eval_metric='mlogloss',\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6        \n",
    "    )\n",
    "    xgb.fit(X_train_np, y_train_np)\n",
    "    print(\"XGBoost training completed\")\n",
    "    \n",
    "    probs = xgb.predict_proba(X_test_np)\n",
    "    preds = xgb.predict(X_test_np)\n",
    "    return preds, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69261e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_dataset(dataset_name, device, embedding_dir, method='xgboost'):\n",
    "    \n",
    "    data_train_path = f\"{embedding_dir}/{dataset_name}_train.pt\"\n",
    "    data_eval_path = f\"{embedding_dir}/{dataset_name}_eval.pt\"\n",
    "    data_test_path = f\"{embedding_dir}/{dataset_name}_test.pt\"\n",
    "\n",
    "    print(f\"[INFO] Loading train data from {data_train_path}\")\n",
    "    train_data = torch.load(data_train_path)\n",
    "    X_train = train_data[\"embeddings\"]\n",
    "    y_train = train_data[\"labels\"]\n",
    "\n",
    "    print(f\"[INFO] Loading validation data from {data_eval_path}\")\n",
    "    eval_data = torch.load(data_eval_path)\n",
    "    X_val = eval_data[\"embeddings\"]\n",
    "    y_val = eval_data[\"labels\"]\n",
    "\n",
    "    print(f\"[INFO] Loading test data from {data_test_path}\")\n",
    "    test_data = torch.load(data_test_path)\n",
    "    X_test = test_data[\"embeddings\"]\n",
    "    y_test = test_data[\"labels\"]\n",
    "\n",
    "    print(f\"\\n[INFO] Label distribution:\")\n",
    "    train_counts = np.bincount(y_train.numpy())\n",
    "    val_counts = np.bincount(y_val.numpy())\n",
    "    test_counts = np.bincount(y_test.numpy())\n",
    "    \n",
    "    print(f\"Train set: {train_counts} (total: {len(y_train)})\")\n",
    "    print(f\"Validation set: {val_counts} (total: {len(y_val)})\")\n",
    "    print(f\"Test set: {test_counts} (total: {len(y_test)})\")\n",
    "    \n",
    "    print(f\"\\n[INFO] Since XGboost does not use validation set, using train and test sets without validation set.\")\n",
    "    print(f\"Train set size: {len(X_train)}\")\n",
    "    print(f\"Test set size: {len(X_test)}\")\n",
    "\n",
    "    y_pred, y_probs = train_xgboost_classifier(\n",
    "            X_train, y_train, X_test,\n",
    "            random_state=42\n",
    "        )\n",
    "    y_true = y_test.numpy()\n",
    "\n",
    "\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "    try:\n",
    "        mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    except:\n",
    "        mcc = 0.0\n",
    "\n",
    "    try:\n",
    "        if y_probs.shape[1] == 2:\n",
    "            auc = roc_auc_score(y_true, y_probs[:, 1])\n",
    "            auprc = average_precision_score(y_true, y_probs[:, 1])\n",
    "        else:\n",
    "            auc = roc_auc_score(y_true, y_probs, multi_class='ovr', average='macro')\n",
    "            auprc = average_precision_score(y_true, y_probs, average='macro')\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating AUC/AUPRC: {e}\")\n",
    "        auc = 0.0\n",
    "        auprc = 0.0\n",
    "\n",
    "    print(f\"\\n[INFO] Per-class accuracy:\")\n",
    "    for class_idx in range(4):\n",
    "        class_mask = (y_true == class_idx)\n",
    "        if np.sum(class_mask) > 0:\n",
    "            class_correct = np.sum((y_pred[class_mask] == class_idx))\n",
    "            class_accuracy = class_correct / np.sum(class_mask)\n",
    "            print(f\"  Class {class_idx}: {class_accuracy:.4f} ({class_correct}/{np.sum(class_mask)})\")\n",
    "        else:\n",
    "            print(f\"  Class {class_idx}: No samples in test set\")\n",
    "\n",
    "    print(f\"\\n[INFO] Error prediction distribution:\")\n",
    "    for true_class in range(4):\n",
    "        true_class_mask = (y_true == true_class)\n",
    "        true_class_indices = np.where(true_class_mask)[0]\n",
    "\n",
    "        if len(true_class_indices) > 0:\n",
    "            preds_for_true_class = y_pred[true_class_mask]\n",
    "            wrong_pred_mask = (preds_for_true_class != true_class)\n",
    "            wrong_preds = preds_for_true_class[wrong_pred_mask]\n",
    "\n",
    "            if len(wrong_preds) > 0:\n",
    "                error_counts = np.bincount(wrong_preds, minlength=4)\n",
    "                total_errors = len(wrong_preds)\n",
    "                print(f\"  For true class {true_class} (errors: {total_errors}/{len(true_class_indices)}):\")\n",
    "                for pred_class in range(4):\n",
    "                    if pred_class != true_class and error_counts[pred_class] > 0:\n",
    "                        percentage = (error_counts[pred_class] / total_errors) * 100\n",
    "                        print(f\"    → Predicted as class {pred_class}: {error_counts[pred_class]} ({percentage:.1f}%)\")\n",
    "            else:\n",
    "                print(f\"  For true class {true_class}: No prediction errors\")\n",
    "        else:\n",
    "            print(f\"  For true class {true_class}: No samples in test set\")\n",
    "\n",
    "    return acc, auc, auprc, f1, mcc, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7aff5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading train data from ./embedding/Genos-1.2B//Human_classify_8k_train.pt\n",
      "[INFO] Loading validation data from ./embedding/Genos-1.2B//Human_classify_8k_eval.pt\n",
      "[INFO] Loading test data from ./embedding/Genos-1.2B//Human_classify_8k_test.pt\n",
      "\n",
      "[INFO] Label distribution:\n",
      "Train set: [50 50 50] (total: 150)\n",
      "Validation set: [50 50 50] (total: 150)\n",
      "Test set: [50 50 50] (total: 150)\n",
      "\n",
      "[INFO] Since XGboost does not use validation set, using train and test sets without validation set.\n",
      "Train set size: 150\n",
      "Test set size: 150\n",
      "Training XGBoost classifier...\n",
      "XGboost parameters: n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42\n",
      "XGBoost training completed\n",
      "\n",
      "[INFO] Per-class accuracy:\n",
      "  Class 0: 0.1800 (9/50)\n",
      "  Class 1: 0.2800 (14/50)\n",
      "  Class 2: 0.4200 (21/50)\n",
      "  Class 3: No samples in test set\n",
      "\n",
      "[INFO] Error prediction distribution:\n",
      "  For true class 0 (errors: 41/50):\n",
      "    → Predicted as class 1: 20 (48.8%)\n",
      "    → Predicted as class 2: 21 (51.2%)\n",
      "  For true class 1 (errors: 36/50):\n",
      "    → Predicted as class 0: 18 (50.0%)\n",
      "    → Predicted as class 2: 18 (50.0%)\n",
      "  For true class 2 (errors: 29/50):\n",
      "    → Predicted as class 0: 15 (51.7%)\n",
      "    → Predicted as class 1: 14 (48.3%)\n",
      "  For true class 3: No samples in test set\n",
      "\n",
      "Completed Human_classify_8k task with XGboost: Acc=0.2933, AUC=0.5065, F1=0.2877\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# XGboost classification main workflow\n",
    "# -------------------------------\n",
    "datasets = [\"Human_classify_8k\"]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    acc, auc, auprc, f1, mcc, precision, recall = evaluate_model_on_dataset(dataset, \n",
    "                                                device, embedding_dir, method='xgboost')\n",
    "print(f\"\\nCompleted {dataset} task with XGboost: Acc={acc:.4f}, AUC={auc:.4f}, F1={f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
